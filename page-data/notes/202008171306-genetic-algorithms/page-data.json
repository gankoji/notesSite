{"componentChunkName":"component---node-modules-gatsby-theme-andy-src-templates-note-js","path":"/notes/202008171306-genetic-algorithms","result":{"data":{"brainNote":{"slug":"202008171306-genetic-algorithms","title":"202008171306-genetic-algorithms","childMdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Genetic Algorithms are generally classified as one type of optimization solver. The idea is effectively to randomly select starting points within the 'search space' of the optimization problem, and randomly morph them over time. At each iteration, the best samples are selected via a fitness function, and those selected are carried over to the next 'generation', where they are again mutated and the selection process started over. Over many generations, this leaves us with a highly 'fit' population of samples, from which we can draw the best sample or examine the whole population to investigate as possible solutions to the original question. \"), mdx(\"p\", null, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"/notes/20200620205040-dissertation-topics-ideas\"\n  }), \"20200620205040-dissertation_topics_ideas\")));\n}\n;\nMDXContent.isMDXComponent = true;"},"inboundReferenceNotes":[{"title":"202008171303-more-on-reinforcement-learning","slug":"202008171303-more-on-reinforcement-learning","childMdx":{"excerpt":"I thought through this quite a bit last night, and I came up with some more questions that I don't necessarily have answers to, at the…"}},{"title":"202008171309-simulated-annealing","slug":"202008171309-simulated-annealing","childMdx":{"excerpt":"Simulated Annealing is another global optimization strategy, and is actually quite similar to  202008171306 Genetic Algorithms . The core…"}}],"outboundReferenceNotes":[{"title":"20200620205040-dissertation-topics-ideas","slug":"20200620205040-dissertation-topics-ideas","childMdx":{"excerpt":"Seems I never got around to doing anything here. Obviously, I need to start fleshing out a list of ideas for the dissertation. Jarvis is one…"}}]},"site":{"siteMetadata":{"title":"Jake's Notes"}}},"pageContext":{"slug":"202008171306-genetic-algorithms"}},"staticQueryHashes":[]}