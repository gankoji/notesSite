{"componentChunkName":"component---node-modules-gatsby-theme-andy-src-templates-note-js","path":"/notesSite/notes/optimal-control","result":{"data":{"brainNote":{"slug":"optimal-control","title":"optimal-control","childMdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"optimal-control\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Optimal control is a broad sub-field of control theory, which covers problems of the type \\\"given a system with specified dynamics, constraints, and goals, find a controller that minimizes/maximizes some cost/benefit.\\\"\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Traditionally, the first time a student experiences this is by the\\ndevelopment of the Linear-Quadratic Regulator, and it's stochastic counterpart the Linear-Quadratic Gaussian Regulator. Both of these involve stating a cost that is quadratic in both the control effort and the state error, and ultimately require solution to the algebraic Riccatti equation in order to form a controller. While this is a beautiful result, it is unfortunately limited to systems with linear dynamics, and thus not applicable to much outside of academia.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The sad realization, post Linear-Quadratic Regulator, is that it's really our only closed form solution in the optimal control design space. As with Optimization in general, very rarely (and only for the simplest problems) are we able to find an analytical solution to a given problem. Most of the time, we must turn to numerical solution of a difficult, coupled system of equations.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Thus, we turn to the more general problem of numerically solving optimal control problems. Betts' book \\\"Practical Methods for Optimal Control and Estimation Using Nonlinear Programming\\\" has the best explanation of this process that I've seen to date. Because the optimal control problem is dynamic, and the dynamics of the system depend on the path and controls taken, we have to solve for the trajectory and control history simultaneously.\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Additionally, because these two quantities are varying in time, but have a time-dependent relationship, we have to solve for the entire histories simultaneously as well.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"This leads to the transcription problem, where we must transform the optimal control problem to a traditional Optimization problem. There are many ways to achieve this: simple discretization, multiple shooting, pseudospectral methods, etc.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The important technique in these methods is to add the dynamics of the system as a constraint on the derivatives of the state trajectory, i.e. dx/dt = f(x,u,t).\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Armed with these dynamic constraints, along with any other state or\\ncontrol constraints, we then have a full blown NLP problem: min J(x,u,t), subject to dx/dt = f(x,u,t), g(x) <= 0, h(x) = 0, with p x n degrees of freedom in the state and p x m degrees of freedom in the control.\")))));\n}\n;\nMDXContent.isMDXComponent = true;"},"inboundReferenceNotes":[],"outboundReferenceNotes":[]},"site":{"siteMetadata":{"title":"Jake's Notes"}}},"pageContext":{"slug":"optimal-control"}},"staticQueryHashes":[]}