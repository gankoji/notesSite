{"componentChunkName":"component---node-modules-gatsby-theme-andy-src-templates-note-js","path":"/\\cloud-backup","result":{"data":{"brainNote":{"slug":"cloud-backup","title":"cloud-backup","childMdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"Keeping Offsite Backups\"), mdx(\"p\", null, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cmoving-data-to-the-cloud\"\n  }), \"Moving data to the cloud\"), \" is an atrocious experience, mostly because of the fact that American ISPs are the worst in the developed world. But hey, what's new? Sadly, this is the way it's going to be for now, because I don't have any better strategies for getting our stuff up into Azure storage. \"), mdx(\"h2\", null, \"Delta Backups are a thing, y'know...\"), mdx(\"p\", null, \"However, they're slightly more complicated than regular deep copies. In order to get a delta copy up to azure storage, I would need a manifest of \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"all the files\"), \" in the original backup, as well as the delta manifests of all those that have come before the one I'm doing right now, and hashes of the file contents. Then, and only then, could I go through the current state of the backup directory, file by file, comparing against manifests and hashes of what's in Azure, compiling a list of what new to compress and upload, creating a manifest there, etc etc. \"), mdx(\"h2\", null, \"Not that this is impossible\"), mdx(\"p\", null, \"But it is significantly more difficult. The difficulty/complexity of this application is compounded by the fact that I'm already running up against data chunking due to the size of the files versus the width of the pipe. Interesting question: \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cdoes-chunking-compound-complexity\"\n  }), \"Does chunking compound complexity\"), \", or increase it additively? I originally thought the former, but as I type it out I'm leaning toward the latter. I suppose I should read on how to reassemble a file that's been chunked via \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cgnu-split\"\n  }), \"GNU Split\"), \".\"), mdx(\"h2\", null, \"Homing in on a design?\"), mdx(\"p\", null, \"I've spent a handful of moments over the last month or two thinking through this problem, but now that I'm actually putting it into practice, I think it's time we worked on the \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cdesign-for-v20\"\n  }), \"Design for v2.0\"), \". \"));\n}\n;\nMDXContent.isMDXComponent = true;"},"inboundReferenceNotes":[{"title":"automating-the-boring-stuff","slug":"automating-the-boring-stuff","childMdx":{"excerpt":"Make it Automatic Manual Backups are not a strategy  states the issue succinctly. So what's the solution? Well, to automate it, obviously…"}},{"title":"manual-backups-are-not-a-strategy","slug":"manual-backups-are-not-a-strategy","childMdx":{"excerpt":"Manual Backups are Not a Strategy Because, honestly, they never happen often enough. Looking back at my latest manual backups, I think the…"}}],"outboundReferenceNotes":[{"title":"moving-data-to-the-cloud","slug":"moving-data-to-the-cloud","childMdx":{"excerpt":"Moving Terabytes to the Cloud SUCKS I finally got a rough and tumble version of my home backup program running, and it is taking FOREVER. I…"}},{"title":"does-chunking-compound-complexity","slug":"does-chunking-compound-complexity","childMdx":{"excerpt":""}},{"title":"gnu-split","slug":"gnu-split","childMdx":{"excerpt":"GNU Split Is super effective! It does exactly what you'd expect it to, given the name: splits up an input file into chunks, according to the…"}},{"title":"design-for-v20","slug":"design-for-v20","childMdx":{"excerpt":""}}]},"site":{"siteMetadata":{"title":"Jake's Notes"}}},"pageContext":{"slug":"cloud-backup"}},"staticQueryHashes":[]}