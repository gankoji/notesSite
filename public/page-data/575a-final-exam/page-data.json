{"componentChunkName":"component---node-modules-gatsby-theme-andy-src-templates-note-js","path":"/\\575a-final-exam","result":{"data":{"brainNote":{"slug":"575a-final-exam","title":"575a-final-exam","childMdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"Final Exam\"), mdx(\"p\", null, \"As requested, I'm writing out my solution approaches to each of the five questions on the final exam shared with me: \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5C2020-4-math-575a-final-exampdf\"\n  }), \"2020-4\", mdx(\"strong\", {\n    parentName: \"a\"\n  }, \"math_575a\"), \"final_exam.pdf\"), \". \"), mdx(\"h2\", null, \"Quantization Effects\"), mdx(\"p\", null, \"I'll admit to being a bit stumped by this one. I don't have an exact answer for why the step size is smaller on the log-log plot when the input is less than 1, but I would hazard a guess that it is due to truncation error of the repeated floating point operations (i.e. when the input is above 1, the square root is repeatedly decreasing the fraction, and thus we're losing bits of precision off to the right, which don't come back when squared). This effect on the fraction goes \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"the other direction\"), \" when the input is less than zero, since square roots increase the magnitude of the fraction for those numbers. \"), mdx(\"p\", null, \"I found the width of the step size for $log_{10}(x)$ 'empirically', simply by running the map on a logspace of inputs and plotting input vs output. Matplotlib's plots have a cursor which lets you 'measure', to a rough precision, points on the graph. The step shown below starts at roughly $x=1.6499$, and ends at $x=2.11526$, for a width of $0.46527$, or a logarithmic difference of  $0.10788$. \"), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"%5C50shadesofrootspng\",\n    \"alt\": \"50shadesofroots.png\"\n  }))), mdx(\"p\", null, \"Code and output for this answer is shown below. This (and all other code shown in this document) was run on Python 3.7.6, with NumPy 1.18.1, on a Windows 10 desktop machine with an AMD Ryzen 3900x, 12 cores @ 4.2 GHz, and 32 GB RAM. \"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"## Question 1, quantization errors\\n## Map is ((x)^(1/(2**50)))^(2**50)\\ndef weirdMap(x):\\n    for i in range(0,50):\\n        x = math.sqrt(x)\\n    \\n    for i in range(0,50):\\n        x = x**2\\n\\n    return x\\n\\nx = np.logspace(-1,1,10000)\\ny = [weirdMap(a) for a in x]\\nplt.loglog(x,y)\\nplt.show()\\n\\nx1 = 1.64999\\nx2 = 2.11526\\n\\nprint(f\\\"Difference: {x2-x1}. Log Difference: {math.log10(x2/x1)}.\\\")\\n\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"Difference: 0.4652700000000001. Log Difference: 0.10788244475855745.\\nTotal time for question 1: 0.20099949836730957.\\n\")), mdx(\"h2\", null, \"Ring of Vibrating Masses\"), mdx(\"p\", null, \"This is a classic problem in many degree of freedom vibrating systems, and is solved via an approach known as modal analysis. As suggested in the problem statement, we begin by writing the equations of motion in matrix form: Mx'' = Kx, where M and K are the mass and stiffness matrices, respectively. \"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"M = diag(1,4,1,4,1,4,1,4,1,4,1,4);\\nK = [-2 1 0 0 0 0 0 0 0 0 0 1;\\n     1 -2 1 0 0 0 0 0 0 0 0 0;\\n     0 1 -2 1 0 0 0 0 0 0 0 0;\\n     ...\\n     1 0 0 0 0 0 0 0 0 0 1 -2];\\n\")), mdx(\"p\", null, \"Then, we compute the mass normalized stiffness matrix, and solve the symmetric eigenvalue problem for this matrix to get the squared natural frequencies (the eigenvalues) and the mode shapes (the eigenvectors). \"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"rootMinv = inv(chol(M));\\nK' = rootMinv*K*rootMinv;\\nD, V = eig(K');\\nomega = sqrt(D);\\nomega\\nV\\n\\n\")), mdx(\"p\", null, \"INCOMPLETE: The exam sheet actually explicitly mentions that using built-ins for 'solving eigenproblems' is disallowed. Thus, we have to write our own procedure for solving the symmetric eigenvalue problem. I think this is a good spot for me to hold up my hands and say \\\"hey, I'm only \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"outlining\"), \" a solution process here, not solving the problem!\\\" To write said eigenproblem solver, I would fall back to my trusty reference \\\"Numerical Recipes: The Art of Scientific Computing\\\" by Press et al. Specifically, Chapter 11, section 4.2, pages 588-589, the QL method for tridiagonal matrices. \"), mdx(\"p\", null, \"This overall solution process (sans eigensolver) is exactly as given in section 4.3 of Inman's \\\"Engineering Vibration\\\", a classic text on the subject. \"), mdx(\"h3\", null, \"After Consultation...\"), mdx(\"p\", null, \"I put together a solution using the eigensolver built-in to NumPy's LinAlg library, just to show the method works. Code and output are listed below. \"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"## Question 2: Modal Analysis\\ndef question2():\\n    M = np.diag([1,4,1,4,1,4,1,4,1,4,1,4])\\n    K = np.array([[-2,1,0,0,0,0,0,0,0,0,0,1],\\n                  [1,-2,1,0,0,0,0,0,0,0,0,0],\\n                  [0,1,-2,1,0,0,0,0,0,0,0,0],\\n                  [0,0,1,-2,1,0,0,0,0,0,0,0],\\n                  [0,0,0,1,-2,1,0,0,0,0,0,0],\\n                  [0,0,0,0,1,-2,1,0,0,0,0,0],\\n                  [0,0,0,0,0,1,-2,1,0,0,0,0],\\n                  [0,0,0,0,0,0,1,-2,1,0,0,0],\\n                  [0,0,0,0,0,0,0,1,-2,1,0,0],\\n                  [0,0,0,0,0,0,0,0,1,-2,1,0],\\n                  [0,0,0,0,0,0,0,0,0,1,-2,1],\\n                  [1,0,0,0,0,0,0,0,0,0,1,-2]])\\n\\n    K = -K\\n    rootMinv = np.linalg.inv(np.linalg.cholesky(M))\\n    Kbar = np.dot(np.dot(rootMinv, K), rootMinv)\\n    w, v = np.linalg.eig(Kbar)\\n    freqs = np.sqrt(w)\\n    freqs[::-1].sort()\\n\\n    for i in range(0, len(freqs)):\\n        print(f\\\"Natural Frequency {i}: {freqs[i]}.\\\")\\n\\nt0 = time.time()\\nquestion2()\\nt1 = time.time()\\nprint(f\\\"Total time for question 2: {t1-t0}\\\")\\n\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"Natural Frequency 0: 1.5811388300841898.\\nNatural Frequency 1: 1.5477867823892801.\\nNatural Frequency 2: 1.54778678238928.\\nNatural Frequency 3: 1.4667609958224272.\\nNatural Frequency 4: 1.4667609958224268.\\nNatural Frequency 5: 1.4142135623730956.\\nNatural Frequency 6: 0.7071067811865475.\\nNatural Frequency 7: 0.5904338922639883.\\nNatural Frequency 8: 0.590433892263988.\\nNatural Frequency 9: 0.3230419109976909.\\nNatural Frequency 10: 0.3230419109976909.\\nNatural Frequency 11: 5.268356063861754e-09.\\nTotal time for question 2: 0.009000778198242188\\n\")), mdx(\"h2\", null, \"Solution of System of Nonlinear Equations\"), mdx(\"p\", null, \"Solving a system of nonlinear equations is, sadly, provably at least as difficult as solving an optimization problem. Sans an analytical solution, we are left with iterative methods. Globally convergent methods are generally desirable for this type of problem, and line search with backtracking is one such solution with good convergence properties. We are guaranteed with this method to converge to \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"a\"), \" minimum of f = 1/2F(x)^2, where F is the vector valued function we seek to solve for F(x) = 0. However, we're not guaranteed to find a zero, so we check for a false solution and start again from a new point if necessary. \"), mdx(\"p\", null, \"The common solution to this problem, although not necessarily the best in terms of performance or convergence guarantees, is the Newton-Rhapson method. This is the (admittedly simplistic or 'easy way out') solution that I present below. Here, F is the user supplied function to be zeroed, and J the user supplied function which outputs the Jacobian of said function with respect to its inputs. This solution follows nearly identically from section 9.6 of \\\"Numerical Recipes: The Art of Scientific Computing\\\" by Press et al. \"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"def newtonRhapsonND(guess, F, J):\\n    xstar = np.zeros(guess.shape)\\n    xstar = guess\\n    ntrial = 100\\n\\n    tolf = 1e-9\\n    tolx = 1e-9\\n\\n    for k in range(0,ntrial):\\n        fvec = F(xstar)\\n        fjac = J(xstar)\\n        errf = np.sum(np.abs(fvec))\\n        if errf < tolf:\\n            return xstar\\n        \\n        # Solve for next step\\n        # J*dx = -F\\n        dx = np.linalg.solve(fjac, -fvec)\\n        errx = np.sum(np.abs(dx))\\n        xstar += dx\\n        if errx < tolx:\\n            return xstar\\n\\n    return xstar\\n    \\ndef myF(x):\\n    a = x[0]\\n    b = x[1]\\n    c = x[2]\\n\\n    out = np.zeros((3,))\\n\\n    out[0] = a - 3*b + 5*c - a**2 - b**2 - c**2\\n    out[1] = ((1/3.0)*a + (27.0/5.0)*b - (125.0/21.0)*c\\n            - 0.5*a**2 - (a + c)*b)\\n    out[2] = (-(1/15.0)*a + (27.0/7.0)*b + (125.0/9.0)*c\\n            - a*(b + c))\\n    return out\\n\\ndef myJac(x):\\n    fjac = np.zeros((3,3))\\n    fjac[0,0] = 1 - 2*x[0]\\n    fjac[0,1] = -3 - 2*x[1]\\n    fjac[0,2] = 5 - 2*x[2]\\n    fjac[1,0] = (1/3.0) - x[0]\\n    fjac[1,1] = (27.0/5.0) - (x[0] + x[2])\\n    fjac[1,2] = -(125.0/21.0) - x[1]\\n    fjac[2,0] = -(1/15.0) - (x[1] + x[2])\\n    fjac[2,1] = (27.0/7.0) - x[0]\\n    fjac[2,2] = (125.0/9.0) - x[0]\\n\\n    return fjac\\n\\nxstar = newtonRhapsonND(np.array([1.0,1.0, 1.0]), myF, myJac)\\nprint(f\\\"Final optimal solution is ({xstar[0]}, {xstar[1]}, {xstar[2]}).\\\")\\n\")), mdx(\"p\", null, \"Output from NumPy:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"Final solution is (0.9153081002598689, 0.024298671214824253, -0.0008064184960937964).\\nTotal time for Newton-Rhapson: 0.0010061264038085938\\n\")), mdx(\"h2\", null, \"Rocket Equation\"), mdx(\"p\", null, \"This one is straightforward. This is a single spatial degree of freedom IVP, for which standard numerical ODE integration methods, such as Runge-Kutta, are more than appropriate. A method such as \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"ode45\"), \" in MATLAB would allow us to simply give the set of ODEs as input (along with an option for the specified absolute tolerance), but that's disallowed. Thus, we do it manually. The code below uses the constant time step fourth-order Runge-Kutta method, which is painfully easy to write. We use an iterative refinement loop to reach the specified accuracy.  \"), mdx(\"p\", null, \"We write out the equations of motion as the functions given in the problem statement:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"def derivativesq4(t, yn):\\n    m = yn[0]\\n    v = yn[1]\\n    r = yn[2]\\n    dm = -1\\n    dv = 10 - m/(r**2)\\n    dr = v\\n    return np.array([dm, dv, dr])\\n\")), mdx(\"p\", null, \"Then, we use the 4th order Runge-Kutta method to integrate the system:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"# Fourth order Runge-Kutta solver\\ndef integrate(yn1, tn1, dt, derivatives):\\n    k1 = dt*derivatives(tn1, yn1)\\n    k2 = dt*derivatives(tn1 + 0.5*dt, yn1 + 0.5*k1)\\n    k3 = dt*derivatives(tn1 + 0.5*dt, yn1 + 0.5*k2)\\n    k4 = dt*derivatives(tn1 + dt, yn1 + k3)\\n\\n    yn = np.zeros((3,))\\n    for i in range(0,3):\\n        yn[i] = yn1[i] + (1/6.0)*k1[i] + (1/3.0)*k2[i] + (1/3.0)*k3[i] + (1/6.0)*k4[i]\\n\\n    return yn\\n\")), mdx(\"p\", null, \"Finally, we wrap the process in two loops. The inner loop simply iterates over the requisite number of time steps to reach the desired end time, given the current time step. The outer loop successively halves the time step until the difference between the final exit velocity from one loop iteration to the next is less than the specified error tolerance (in this case, we actually only need 1E-6, since the answer is of magnitude 1E2, but we go to 1E-8 to be thorough). \"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"def question4():\\n    oldVe = 1e6\\n    currVe = 0\\n    eps = 1e-8\\n\\n    dt = 2\\n    tf = 9\\n    nsteps = math.floor(tf/dt) + 1\\n    y = np.zeros((nsteps, 3))\\n\\n    m0 = 10\\n    v0 = 0\\n    r0 = 1\\n\\n    while (math.fabs(currVe - oldVe) > eps):\\n        dt = dt/2\\n        nsteps = math.floor(tf/dt) + 1\\n\\n        y = np.zeros((nsteps,3))\\n        time = np.linspace(0,tf, nsteps)\\n        y[0,0] = m0\\n        y[0,1] = v0\\n        y[0,2] = r0\\n\\n        for i in range(1, nsteps):\\n            y[i] = integrate(y[i-1], time[i-1], dt, derivativesq4)\\n\\n        oldVe = currVe\\n        currVe = math.sqrt(y[-1,1]**2 - (2/y[-1,2]))\\n\\n    print(\\\"Final exit velocity: \\\" + str(currVe))\\n\\n    plt.plot(time, y[:,1])\\n    plt.show()\\n\\n\")), mdx(\"p\", null, \"Since NumPy uses double precision floats by default, we are unlikely to run into difficulty with compounding truncation errors while reaching the requested accuracy threshold. This code takes approximately 0.29s to run on my machine. \"), mdx(\"p\", null, \"Output from NumPy:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"Final exit velocity: 20.5351500922157\\nTotal time for question 4: 0.07754755020141602\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"%5Crocketvelocitypng\",\n    \"alt\": \"RocketVelocity.png\"\n  }))), mdx(\"h2\", null, \"Question 5 - BVP\"), mdx(\"p\", null, \"The most straightforward approach to solving BVPs is a method known as the shooting method. After solving question 4 above, I was quite excited to simply power on and solve this one directly as well. However, once I remembered the constraints of this exam (no IVP/BVP solvers for ODEs, obviously, but also no fsolve or similar for nonlinear systems!), I realized the amount of work necessary would be quite a bit. I'm still working the solution for fun on my own time, but this is the procedure we'll follow:\"), mdx(\"p\", null, \"1) Rewrite the given ODE as a system of first-order ODEs, e.g. $$y\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"1' = y_2$$ $$ y_2' = y_1^3 - y$$\", mdx(\"br\", {\n    parentName: \"em\"\n  }), \"2) Initialize this system with the one given 'initial' boundary condition, i.e. $$y_1(0) = 1$$ Treat the second initial value y2(0) as a freely specified variable (we'll call it V).\\n3) These next steps are performed iteratively, until the desired solution accuracy is reached.\\n1) Integrate the system with a suitable method (RK4 above works well) from the generated initial conditions $$ \\\\hat{y}_0 = \", \"[y(0), V]\", \"$$\\n2) 'Score' the solution according to the discrepancy at the 'other end', i.e. $$F = y_1(20) + 1$$\\n3) If |F| is sufficiently small, stop.\\n4) Find an adjustment to V as $$V\"), \"{new} = V\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"{old} + dV$$ $$JdV = -F$$ J here is the Jacobian of F wrt V, eg $$J\"), \"{ij} = \\\\frac{dF_i}{dV_j}$$ These elements of the Jacobian must be solved for with dim(V) more integrations of the ODE system, again via RK4 or similar.\\n5) Repeat from step 3.1 with new initial conditions: $$ \\\\hat{y}_0 = \", \"[y_1(0), Vnew]\", \"$$\"), mdx(\"p\", null, \"The above described procedure is the shooting method, utilizing the Newton-Rhapson procedure to iteratively solve for the set of 'initial' boundary values which cause F to be (near) zero. By adding a function to the code from question 3 to automatically differentiate F to generate J (with more forward integration from perturbed starting points), we could solve this quite readily. Details can be found in chapter 18 of \\\"Numerical Recipes: The Art of Scientific Computing\\\" by Press et al. \"), mdx(\"h2\", null, \"Actually, we have a solution...\"), mdx(\"p\", null, \"After writing that last paragraph, I decided it wasn't terribly difficult to simply go ahead and amend the Newton-Rhapson procedure from question 3 and went ahead and solved the problem. Code, output, and plot are below. I did have to play with the initial guess and the tuning for the numerical Jacobian procedure a bit, as this equation seems quite stiff. A better ODE integration technique with an adaptive step size would likely have made this significantly easier. \"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"## Problem 5: BVP \\n## y'' = y^3 - y, y(0) = 1, y(20) = -1\\n\\n## Here we use the shooting method. First, decompose to two first order odes\\n## y1' = y2\\n## y2' = y1^3 - y1\\n## IVs: y1[0] = 1, y2[0] = free\\n## Integrate with RK4, check y1[20], correct and repeat. \\n\\ndef derivativesq5(t, yn):\\n    out = np.zeros((2,))\\n    out[0] = yn[1]\\n    out[1] = yn[0]**3 - yn[0]\\n\\n    return out\\n\\ndef Score_q5(v):\\n    nsteps = 5000\\n    #print(v)\\n    x = np.linspace(0,20,nsteps)\\n    dx = x[2] - x[1]\\n    y = np.zeros((nsteps,2))\\n    y[0,0] = 1\\n    y[0,1] = v\\n\\n    for i in range(1, nsteps):\\n        y[i,:] = integrate(y[i-1,:], x[i-1], dx, derivativesq5, 2)\\n\\n    out = y[-1,0] + 1\\n    # plt.plot(x,y)\\n    # plt.show()\\n    return np.array([out], dtype=np.dtype(np.float64))\\n\\ndef Jacobian_q5(v):\\n    # Complex step differentiation would likely be much more accurate here\\n    vnew = v*2.0\\n    perturb = vnew-v\\n\\n    out = (Score_q5(vnew) - Score_q5(v))/perturb\\n    return np.array([out], dtype=np.dtype(np.float64))\\n\\ndef question5():\\n    #Score_q5(-0.01)\\n    xstar = newtonRhapsonND(np.array([-0.0000001]), Score_q5, Jacobian_q5)\\n    nsteps = 5000\\n    x = np.linspace(0,20,nsteps)\\n    dx = x[2] - x[1]\\n    y = np.zeros((nsteps, 2))\\n    y[0,0] = 1\\n    y[0,1] = xstar[0]\\n\\n    for i in range(1, nsteps):\\n        y[i,:] = integrate(y[i-1,:], x[i-1], dx, derivativesq5, 2)\\n\\n    if plots:\\n        plt.plot(x,y)\\n        plt.xlabel(\\\"X\\\")\\n        plt.ylabel(\\\"Y\\\")\\n        plt.title(\\\"y'' = y^3 - y, y(0) = 1, y(20) = -1\\\")\\n        plt.legend([\\\"Y_1\\\", \\\"Y_2\\\"])\\n        plt.show()\\n\\nt0 = time.time()\\nquestion5()\\nt1 = time.time()\\nprint(f\\\"Total time for question 5: {t1-t0}.\\\")\\n\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"Total time for question 5: 12.914722204208374.\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"%5Cbvppng\",\n    \"alt\": \"BVP.png\"\n  }))));\n}\n;\nMDXContent.isMDXComponent = true;"},"inboundReferenceNotes":[],"outboundReferenceNotes":[{"title":"2020-4-math-575a-final-exampdf","slug":"2020-4-math-575a-final-exampdf","childMdx":{"excerpt":""}},{"title":"50shadesofrootspng","slug":"50shadesofrootspng","childMdx":{"excerpt":""}},{"title":"rocketvelocitypng","slug":"rocketvelocitypng","childMdx":{"excerpt":""}},{"title":"bvppng","slug":"bvppng","childMdx":{"excerpt":""}}]},"site":{"siteMetadata":{"title":"Jake's Notes"}}},"pageContext":{"slug":"575a-final-exam"}},"staticQueryHashes":[]}