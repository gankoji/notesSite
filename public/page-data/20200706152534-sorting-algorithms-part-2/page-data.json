{"componentChunkName":"component---node-modules-gatsby-theme-andy-src-templates-note-js","path":"/20200706152534-sorting-algorithms-part-2","result":{"data":{"brainNote":{"slug":"20200706152534-sorting-algorithms-part-2","title":"20200706152534-sorting-algorithms-part-2","childMdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"20200706152534-sorting-algorithms-part-2\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Looking at the b-tree sort algorithm again. By not taking the naive approach and using binary search on each node for an insertion, we can reduce our number of comparisons in the worst case from m\", \"*\", \"h (m items per node, h nodes high tree) to log2(m)\", \"*\", \"h. In the case considered here, with m=200 and thus h either 4 or 5, that leaves us with 39 comparisons per insertion in the worst case. In reality, it will be \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"much\"), \" better for most elements, since the worst case doesn't happen until we start getting a very full tree. We're really looking at building a sorted tree in something like m\", \"*\", \"log2(m)/2 time, if we only consider comparisons. Of course, as previously mentioned, since this tree isn't going to come anywhere close to fitting in main memory, we're going to have to worry about disk access during inserts. We're very likely going to need to manually manage some sort of cache for all the nodes we're working with. LRU isn't a great strategy here, though, since the keys are going to be randomly generated (and likely with a uniform distribution). Thus, the least recently used node is highly \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"unlikely\"), \" to be needed again for the next insertion. Can we do a prefetch on the nodes that are referenced by a given node? Would that just spin the disks unnecessarily?\"), mdx(\"p\", null, \"Maybe the strategy here is to keep the top levels of nodes in memory, so we can reduce the number of disk accesses to one or two max each insertion. Now that I consider that, we're also going to have to have another disk access at each insertion to write the record out! Those, at the very least, need to be buffered and sent in batches, probably in a separate thread. Tuning that cache size, vs the nodes in memory cache, is going to be a bit of a juggling act.\"), mdx(\"p\", null, \"Maybe I get around that by doing the tree creation in place, keeping pointers to the records themselves in the tree, and only doing the reads and writes once the tree is completely built? But then I have the trouble of trying to organize those seeks, reads, and rewrites in a buffered/cached way. No matter what, I'm going to have to get down and dirty with the disks here. It's just a matter of when it's best to do it, which I'm unsure of at the moment. Let's consider a smaller problem size first, and see if we can work our way up to the terabytes range.\"), mdx(\"p\", null, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"20200714211751-sorting_algorithms_part_3\"\n  }), \"Sorting Algorithms part 3\")));\n}\n;\nMDXContent.isMDXComponent = true;"},"inboundReferenceNotes":[],"outboundReferenceNotes":[]},"site":{"siteMetadata":{"title":"Jake's Notes"}}},"pageContext":{"slug":"20200706152534-sorting-algorithms-part-2"}},"staticQueryHashes":[]}