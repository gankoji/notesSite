{"componentChunkName":"component---node-modules-gatsby-theme-andy-src-templates-note-js","path":"/\\massive-scale-simd-processors","result":{"data":{"brainNote":{"slug":"massive-scale-simd-processors","title":"massive-scale-simd-processors","childMdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"What is a Massive-Scale SIMD Processor?\"), mdx(\"p\", null, \"SIMD = Single Instruction,  Multiple Data. It's one of the forms of parallelism, and probably the most widely supported and exploited. Intel and AMD long ago started baking instructions of this type into their chips (starting with MMX, then they called SSE or Streaming SIMD Extensions, and now its know as AVX). \"), mdx(\"p\", null, \"What is sort of hard for most folks to grasp about GPU programming is that literally all GPU work is done in this same sort of fashion, but to an even greater extent. In AVX, for instance, you can swap between normal instructions to load up a buffer or register, and then fire off a few SIMD ones to crunch that bundle of data you'd assembled, and then go back to doing what you were doing. On most GPUs, however, you're basically working in this sort of fashion at all times. \"), mdx(\"p\", null, \"Basically, the processing paradigm of a GPU is to take a single set of instructions (that don't branch! unless absolutely necessary), and apply them to as much data as can fit in a given area. So how does this work out for \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cdistributing-computations-in-optimization\"\n  }), \"Distributing Computations in Optimization\"), \"? Pretty simply: you take all of the points you're considering, on this particular run/iteration, jam them into a buffer, and then run the entire set of evaluations you need for that point at once (so like, 4096 points or however many CUDA cores you have, then run a few thousand instructions to iterate through all of your constraint and objective functions, to return a buffer of data with the results for each of the points you chewed on). Super simple, right? Unless, of course, getting the data to or from the GPU is a bottleneck. Same principle applies to distributing those computations across nodes/machines/datacenters as well! But, the question of \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Corchestrating-a-distributed-optimization\"\n  }), \"Orchestrating a Distributed Optimization\"), \" arises, and it's not as easily answered. \"));\n}\n;\nMDXContent.isMDXComponent = true;"},"inboundReferenceNotes":[{"title":"heterogeneous-computing-in-optimization","slug":"heterogeneous-computing-in-optimization","childMdx":{"excerpt":"GPUs are the Answer to Most Questions Not really, but they're pretty freaking slick at doing tons of the same computation on different…"}}],"outboundReferenceNotes":[{"title":"distributing-computations-in-optimization","slug":"distributing-computations-in-optimization","childMdx":{"excerpt":"The True Key to Success The true difficulty in optimization problems is not just in their size (most optimization problems are NP Hard for…"}},{"title":"orchestrating-a-distributed-optimization","slug":"orchestrating-a-distributed-optimization","childMdx":{"excerpt":""}}]},"site":{"siteMetadata":{"title":"Jake's Notes"}}},"pageContext":{"slug":"massive-scale-simd-processors"}},"staticQueryHashes":[]}