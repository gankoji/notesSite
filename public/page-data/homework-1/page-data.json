{"componentChunkName":"component---node-modules-gatsby-theme-andy-src-templates-note-js","path":"/\\homework-1","result":{"data":{"brainNote":{"slug":"homework-1","title":"homework-1","childMdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"homework-1\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"Homework 1 - Math 575B\"), mdx(\"h3\", null, \"Jake Bailey, 16 Jan 2021\"), mdx(\"p\", null, \"Questions 16.1, 16.2, 17.1, 17.2 from the course notes\"), mdx(\"h2\", null, \"Question 16.1\"), mdx(\"p\", null, \"Consider the problem of fitting the cloud of points $(x\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"i,y_i)$, $1 \\\\leq i \\\\leq N$, by a linear function $y = ax+b$. The fit minimizes the sum of squares $\\\\sum\\\\limits^N\"), \"{i=1}(ax_i+b - y_i)^2$. Write down explicit formulae for a and b. When are the solutions for a and b not unique?\"), mdx(\"h3\", null, \"Solution\"), mdx(\"p\", null, \"Explicit formulae for the linear least squares regression problem are well known in the literature. Here they are in scalar form:\"), mdx(\"p\", null, \"$$m = \\\\frac{\\\\sum(x - \\\\bar{x})(y - \\\\bar{y})}{\\\\sum(x - \\\\bar{x})^2}$$\\n$$ b = \\\\frac{\\\\sum(y) - m\\\\sum(x)}{N}$$\\n$$ y = mx + b$$\"), mdx(\"p\", null, \"The solution to these equations is \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"not unique\"), \" whenever the number of data points ($N$) available is less than the number of degrees of freedom in the regression, in this case 2. It can also be non-unique whenever there are sufficiently many linearly dependent data points (e.g. all along the same line) such that the number of linearly independent points is less than the number of degrees of freedom, i.e. $rank(A) < 2$, where $A$ is the matrix of data points.  \"), mdx(\"h2\", null, \"Question 16.2\"), mdx(\"p\", null, \"Fit the cloud of points $\", \"{\", \"(-2,-3), (-1,-1), (0,5), (2,5), (3,1)\", \"}\", \"$ by $y=ax+b$ line using the least squares method. \"), mdx(\"h3\", null, \"Solution\"), mdx(\"p\", null, \"Code:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"def solve\\\\_least\\\\_squares(x, y):\\n\\n n = x.size\\n m = np.sum((x - np.mean(x))\\\\*(y - np.mean(y)))/(np.sum((x - np.mean(x))\\\\*\\\\*2))\\n b =\\xA0(np.sum(y)\\xA0- m\\\\*np.sum(x))/n\\n\\n coeffs = np.array(\\\\[b, m\\\\])\\n return coeffs\\n\\n  \\n\\ndef q\\\\_16\\\\_2():\\n\\n x = np.array(\\\\[-2.,\\xA0-1., 0., 2., 3.\\\\])\\n y = np.array(\\\\[-3.,\\xA0-1., 5., 5., 1.\\\\])\\n\\n c = solve\\\\_least\\\\_squares(x, y)\\n print(c)\\n\")), mdx(\"p\", null, \"Output\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"[1. 1.]\\n\")), mdx(\"h2\", null, \"Question 17.1\"), mdx(\"p\", null, \"Consider a function $H_2(x,y)=40\\\\sqrt{g(y^2 - x^2)} +(x - 10)^2+y^2$, where $g(x)=\\\\sqrt{x^2+1}+x$. Find the minimum of $H_2$ by the gradient descent method, starting from $(x, y) = (-50, 40)$.\"), mdx(\"h3\", null, \"Solution\"), mdx(\"p\", null, \"Code:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \" def q_17_1():\\n    x, y = symbols('x y')\\n    g = sqrt(x**2 + 1) + x\\n    gxy = g.subs(x, (y**2 - x**2))\\n    h2 = 40*sqrt(gxy) + (x - 10)**2 + y**2\\n    print(f\\\"Objective Function: {h2}\\\")\\n\\n    R = CoordSys3D('R')\\n    h2R = h2.subs([(x, R.x), (y, R.y)])\\n    grad = gradient(h2R)\\n    print(f\\\"Gradient of Objective Function: {grad}\\\")\\n\\n    newGrad = lambdify([x, y], derive_by_array(h2, (x, y)))\\n    newH = lambdify([x, y], h2)\\n\\n    x, y, old_F, F, dt =  -50.0, 40.0, 0., 6000., 0.01\\n    F_x = newGrad(x,y)\\n\\n    print(f\\\"\\\\n\\\\nMinimizing by Gradient Descent. Starting at ({x}, {y}).\\\")\\n    while abs(F - old_F) > 1e-10:\\n        old_x, old_y, old_F, F_x, = x, y, F, newGrad(x,y)\\n        x, y, dt = x - dt*F_x[0], y - dt*F_x[1], 1.1*dt\\n        F = newH(x,y)\\n\\n        if (F > old_F):\\n            x, y, F, old_F, dt = old_x, old_y, old_F, old_F + 1., 0.5*dt\\n\\n    print(\\\"\\\\nSolution Found. \\\")\\n    print(f\\\"Optimal Solution Point: ({x}, {y})\\\")\\n    print(f\\\"Objective Function Value: {F}\\\")\\n\")), mdx(\"p\", null, \"Output:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"Objective Function: y**2 + (x - 10)**2 + 40*sqrt(-x**2 + y**2 + sqrt((-x**2 + y**2)**2 + 1))\\nGradient of Objective Function: (2*R.x + 40*(-R.x*(-R.x**2 + R.y**2)/sqrt((-R.x**2 + R.y**2)**2 + 1) - R.x)/sqrt(-R.x**2 + R.y**2 + sqrt((-R.x**2 + R.y**2)**2 + 1)) - 20)*R.i + (2*R.y + 40*(R.y*(-R.x**2 + R.y**2)/sqrt((-R.x**2 + R.y**2)**2 + 1) + R.y)/sqrt(-R.x**2 + R.y**2 + sqrt((-R.x**2 + R.y**2)**2 + 1)))*R.j\\n\\n\\nMinimizing by Gradient Descent. Starting at (-50.0, 40.0).\\n\\nSolution Found.\\nOptimal Solution Point: (10.137600306590192, -7.567754946275522e-11)\\nObjective Function Value: 2.808936967472269\\n\\n\")), mdx(\"h2\", null, \"Question 17.2\"), mdx(\"p\", null, \"Consider a function $V_2(x,y) = (x+3)^2 +y^2e^{-2x}$. Find the minimum of (a) $V_2(x,y)$ and (b) $W_2(x,z) = V_2(x, y = z/20)$ by the gradient descent method, starting from $(x, y) = (0,1)$ or $(x, z) = (0,20)$. (c) Part (b) can be considered as an application of the steepest descent method to $V_2$. What norm $||\\\\Delta x||$ is used?\"), mdx(\"h3\", null, \"Solution\"), mdx(\"p\", null, \"Code is nearly identical to that used for 17.1 above, so we don't repeat it here. The process is the same: use SymPy for symbolic computation of the gradient of the function to be minimized (which is valid since all of our functions considered here have gradients with analytic form), and use that in the standard gradient decent approach. The output for both cases is presented below. \"), mdx(\"p\", null, \"Output:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"Objective Function: y**2*exp(-2*x) + (x + 3)**2\\nGradient of Objective Function: (2*R.x - 2*R.y**2*exp(-2*R.x) + 6)*R.i + (2*R.y*exp(-2*R.x))*R.j\\n\\n\\nMinimizing by Gradient Descent. Starting at (0, 1.0).\\nSolution Found.\\nOptimal Solution Point: (-2.9994459920799574, 4.14023538097926e-06)\\nObjective Function Value: 3.1383251178242403e-07\\n\\n\\nObjective Function: y**2*exp(-2*x)/400 + (x + 3)**2\\nGradient of Objective Function: (2*R.x - R.y**2*exp(-2*R.x)/200 + 6)*R.i + (R.y*exp(-2*R.x)/200)*R.j\\n\\n\\nMinimizing by Gradient Descent. Starting at (0, 20.0).\\n\\nSolution Found.\\nOptimal Solution Point: (-3.0000062514806602, -9.295022079890573e-07)\\nObjective Function Value: 3.9952401669108044e-11\\n\")), mdx(\"p\", null, \"For part (c), we aren't enforcing a condition on $||\\\\Delta x||$, so I don't see how that comes into play. Since we're not using any sort of transformation, and relying on the scaling of the second argument to ensure the steepest descent condition, we must be utilizing the $L_1$ norm.  \"));\n}\n;\nMDXContent.isMDXComponent = true;"},"inboundReferenceNotes":[],"outboundReferenceNotes":[]},"site":{"siteMetadata":{"title":"Jake's Notes"}}},"pageContext":{"slug":"homework-1"}},"staticQueryHashes":[]}