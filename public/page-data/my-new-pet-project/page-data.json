{"componentChunkName":"component---node-modules-gatsby-theme-andy-src-templates-note-js","path":"/\\my-new-pet-project","result":{"data":{"brainNote":{"slug":"my-new-pet-project","title":"my-new-pet-project","childMdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"This again?\"), mdx(\"p\", null, \"Oh c'mon, it'll be fun! Seriously, though, I have spent a lot of effort thinking on this in the past with \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cgpgpu-productivity\"\n  }), \"GPGPU Productivity\"), \". \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cdistributed-optimization-in-learning\"\n  }), \"Distributed Optimization in Learning\"), \" is a project I was set upon by Dr. Chertkov, sort of unexpectedly, but I think it has great ties to the original thoughts with GPUs, as I note in \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cheterogeneous-computing-in-optimization\"\n  }), \"Heterogeneous Computing in Optimization\"), \". The other key part to this will be the flip side of the coin, getting less specialized and more generic, as in \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cdistributing-computations-in-optimization\"\n  }), \"Distributing Computations in Optimization\"), \". If I can get the right sort of tools together that make it nearly trivial for an analyst (with nearly no computing knowledge outside of her specific domain of interest) to scale her computational power linearly (or better, super-linearly with GPUs) with the number of nodes at her behest, how much bigger of optimization problems could she then tackle? How many problems are today still out of reach of the aero designer, the business analyst, the operations researcher, because they're just too damn big, or too complicated to get the right algorithm in place?  \"), mdx(\"p\", null, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cuofa-applied-math\"\n  }), \"UofA Applied Math\")));\n}\n;\nMDXContent.isMDXComponent = true;"},"inboundReferenceNotes":[{"title":"current-projects","slug":"current-projects","childMdx":{"excerpt":"Top of the Stack GPGPU Productivity \n My New Pet Project \n Dissertation Topic Ideas On the Back Burner The Project Bike Archived Archived…"}},{"title":"my-research-reading","slug":"my-research-reading","childMdx":{"excerpt":"Books I'm Considering, By Topic Virtual Machines, Language Design GPGPU Productivity Virtual Machines, Craig The Garbage Collection Handbook…"}}],"outboundReferenceNotes":[{"title":"gpgpu-productivity","slug":"gpgpu-productivity","childMdx":{"excerpt":"Improving GPGPU This project is aimed at finding out about the latest and greatest in GPGPU technology, looking for tools, platforms, and…"}},{"title":"distributed-optimization-in-learning","slug":"distributed-optimization-in-learning","childMdx":{"excerpt":"Belief Propagation I received an email from Dr. Chertkov this morning saying that he was working on finding a smooth transition path, but…"}},{"title":"heterogeneous-computing-in-optimization","slug":"heterogeneous-computing-in-optimization","childMdx":{"excerpt":"GPUs are the Answer to Most Questions Not really, but they're pretty freaking slick at doing tons of the same computation on different…"}},{"title":"distributing-computations-in-optimization","slug":"distributing-computations-in-optimization","childMdx":{"excerpt":"The True Key to Success The true difficulty in optimization problems is not just in their size (most optimization problems are NP Hard for…"}},{"title":"uofa-applied-math","slug":"uofa-applied-math","childMdx":{"excerpt":"The Journey Begins, Again... I should probably get this whole PhD story down into my notes, so I can remember it for posterity's sake. I…"}}]},"site":{"siteMetadata":{"title":"Jake's Notes"}}},"pageContext":{"slug":"my-new-pet-project"}},"staticQueryHashes":[]}