{"componentChunkName":"component---node-modules-gatsby-theme-andy-src-templates-note-js","path":"/deduplication","result":{"data":{"brainNote":{"slug":"deduplication","title":"deduplication","childMdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"deduplication\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h2\", null, \"Deduplication\"), mdx(\"p\", null, \"Perhaps the most important piece of this project will be the simply stated task of deduplication. Sifting through an entire (potentially enormous) dataset, and checking each file against a hashset of previously seen files to make sure we're not ingesting duplicates. \"), mdx(\"h3\", null, \"The Easy Part\"), mdx(\"p\", null, \"The easy part is what I just described: on ingest to the datastore, check each new file against a hashset of previously ingested files. When the hash is done on the contents of the file, this immediately rejects any new files/objects/blobs that are byte for byte identical to a previous object. This is trivial enough to write in Python in an hour, and even have a fairly high performance tuned implementation done in an afternoon. That said...\"), mdx(\"h3\", null, \"The Hard Part\"), mdx(\"p\", null, \"Is handling objects that are \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"nearly\"), \" the same, but not identical. These will pass the 'hash test' on the contents in their entirety, because files that are different by even one byte will have wildly different hashes. So what is the solution here? I think this will take some research. This research will also be a good opportunity to look at what's on the market as far as personal data management solutions go. \"));\n}\n;\nMDXContent.isMDXComponent = true;"},"inboundReferenceNotes":[{"title":"the-personal-datavault","slug":"the-personal-datavault","childMdx":{"excerpt":"The Data Problem I'm sure this is not a new problem in the world of computer science, but it seems to me that 80 years into this ordeal weâ€¦"}}],"outboundReferenceNotes":[]},"site":{"siteMetadata":{"title":"Jake's Notes"}}},"pageContext":{"slug":"deduplication"}},"staticQueryHashes":[]}