{"componentChunkName":"component---node-modules-gatsby-theme-andy-src-templates-note-js","path":"/\\distributing-computations-in-optimization","result":{"data":{"brainNote":{"slug":"distributing-computations-in-optimization","title":"distributing-computations-in-optimization","childMdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"The True Key to Success\"), mdx(\"p\", null, \"The true difficulty in optimization problems is not just in their size (most optimization problems are NP Hard for exact solutions), but also in the coupling inherent in their many parts. Not even just from a governing equation perspective (in \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Ccontinuous-optimization\"\n  }), \"Continuous Optimization\"), \", it is fairly common to have the dynamics or other constraints of the problem be a set of coupled, nonlinear differential or algebraic equations). In \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Coptimal-control\"\n  }), \"Optimal Control\"), \", it is even more common to encounter coupled, nonlinear \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cdifferential-algebraic-equations\"\n  }), \"Differential-Algebraic Equations\"), \", or DAEs, which are even more difficult in their treatment. \"), mdx(\"p\", null, \"What I thought was most interesting about the paper on \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cbelief-propagation\"\n  }), \"Belief Propagation\"), \" was the presentation of an approximation technique which utilizes factorings/partitionings of \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cgraphs\"\n  }), \"Graphs\"), \" to chunk the overall problem and distribute its computation amongst many players. \"), mdx(\"p\", null, \"The thought process here is, how can we apply this same type of rule in \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Ccontinuous-optimization\"\n  }), \"Continuous Optimization\"), \", and achieve the same sort of distribution? Problems might be intractable on a single computer when the algorithm for exact solution is O(mn) and mn >= 1E9 or so, but is that still the case if you can spread the search across maybe a thousand nodes? Does having so much more computational power increase the size of the problems we can handle, and does it do said scaling \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"at least linearly\"), \" with the number of nodes? \"));\n}\n;\nMDXContent.isMDXComponent = true;"},"inboundReferenceNotes":[{"title":"heterogeneous-computing-in-optimization","slug":"heterogeneous-computing-in-optimization","childMdx":{"excerpt":"GPUs are the Answer to Most Questions Not really, but they're pretty freaking slick at doing tons of the same computation on different…"}},{"title":"massive-scale-simd-processors","slug":"massive-scale-simd-processors","childMdx":{"excerpt":"What is a Massive-Scale SIMD Processor? SIMD = Single Instruction,  Multiple Data. It's one of the forms of parallelism, and probably the…"}},{"title":"my-new-pet-project","slug":"my-new-pet-project","childMdx":{"excerpt":"This again? Oh c'mon, it'll be fun! Seriously, though, I have spent a lot of effort thinking on this in the past with  GPGPU Productivity…"}},{"title":"my-research-reading","slug":"my-research-reading","childMdx":{"excerpt":"Books I'm Considering, By Topic Virtual Machines, Language Design GPGPU Productivity Virtual Machines, Craig The Garbage Collection Handbook…"}}],"outboundReferenceNotes":[{"title":"continuous-optimization","slug":"continuous-optimization","childMdx":{"excerpt":""}},{"title":"optimal-control","slug":"optimal-control","childMdx":{"excerpt":"Optimal control is a broad sub-field of control theory, which covers problems of the type \"given a system with specified dynamics…"}},{"title":"differential-algebraic-equations","slug":"differential-algebraic-equations","childMdx":{"excerpt":""}},{"title":"belief-propagation","slug":"belief-propagation","childMdx":{"excerpt":"Well, that went well... Read through the paper   today, and did my best to understand it. Unfortunately, that wasn't much. There was a lot…"}},{"title":"graphs","slug":"graphs","childMdx":{"excerpt":""}}]},"site":{"siteMetadata":{"title":"Jake's Notes"}}},"pageContext":{"slug":"distributing-computations-in-optimization"}},"staticQueryHashes":[]}