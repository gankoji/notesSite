{"componentChunkName":"component---node-modules-gatsby-theme-andy-src-templates-note-js","path":"/\\heterogeneous-computing-in-optimization","result":{"data":{"brainNote":{"slug":"heterogeneous-computing-in-optimization","title":"heterogeneous-computing-in-optimization","childMdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"GPUs are the Answer to Most Questions\"), mdx(\"p\", null, \"Not really, but they're pretty freaking slick at doing tons of the same computation on different buffers of data. This is actually an amazing insight that I'm surprised I've never hit before: GPUs (and FPGAs with the current \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Copencl\"\n  }), \"OpenCL\"), \" implementations) are \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cmassive-scale-simd-processors\"\n  }), \"Massive-Scale SIMD Processors\"), \". The amazing thing is, in optimization problems, we're often looking for just such a device! We have many small-ish functions, say constraint functions, objective functions, etc that we must evaluate over and over and over (especially when we're using a gradient-less algorithm, and we have to use \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Capproximate-gradients\"\n  }), \"Approximate Gradients\"), \"). If you could get those functions all expressed in a GPU computable format, and do so without troubling the analyst/optimizer with such minutia and expecting him to also be a GPU expert, you might just create the next killer app for heterogeneous computing. \"), mdx(\"p\", null, \"The key insight here is that \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Coptimization-algorithms-rely-deeply-on-function-evaluation\"\n  }), \"Optimization Algorithms Rely Deeply on Function Evaluation\"), \". This is also true of other more generic simulations. The point above, about not needing to be a GPU expert to express your math in GPU friendly format, touches gently on the other project I've been mulling over for months now, \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cgpgpu-productivity\"\n  }), \"GPGPU Productivity\"), \". Can we combine the two here, along with \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"%5Cdistributing-computations-in-optimization\"\n  }), \"Distributing Computations in Optimization\"), \", and create the ultimate in scaling for difficult math problems? That would be pretty wild, but if done right could be \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"massively\"), \" successful. \"));\n}\n;\nMDXContent.isMDXComponent = true;"},"inboundReferenceNotes":[{"title":"my-new-pet-project","slug":"my-new-pet-project","childMdx":{"excerpt":"This again? Oh c'mon, it'll be fun! Seriously, though, I have spent a lot of effort thinking on this in the past with  GPGPU Productivity…"}},{"title":"my-research-reading","slug":"my-research-reading","childMdx":{"excerpt":"Books I'm Considering, By Topic Virtual Machines, Language Design GPGPU Productivity Virtual Machines, Craig The Garbage Collection Handbook…"}}],"outboundReferenceNotes":[{"title":"approximate-gradients","slug":"opencl","childMdx":{"excerpt":"TK"}},{"title":"massive-scale-simd-processors","slug":"massive-scale-simd-processors","childMdx":{"excerpt":"What is a Massive-Scale SIMD Processor? SIMD = Single Instruction,  Multiple Data. It's one of the forms of parallelism, and probably the…"}},{"title":"approximate-gradients","slug":"approximate-gradients","childMdx":{"excerpt":"TK"}},{"title":"optimization-algorithms-rely-deeply-on-function-evaluation","slug":"optimization-algorithms-rely-deeply-on-function-evaluation","childMdx":{"excerpt":"Optimization is Function Evaluation When considering an optimization problem, we state the problem itself as some type of function: Minimize…"}},{"title":"gpgpu-productivity","slug":"gpgpu-productivity","childMdx":{"excerpt":"Improving GPGPU This project is aimed at finding out about the latest and greatest in GPGPU technology, looking for tools, platforms, and…"}},{"title":"distributing-computations-in-optimization","slug":"distributing-computations-in-optimization","childMdx":{"excerpt":"The True Key to Success The true difficulty in optimization problems is not just in their size (most optimization problems are NP Hard for…"}}]},"site":{"siteMetadata":{"title":"Jake's Notes"}}},"pageContext":{"slug":"heterogeneous-computing-in-optimization"}},"staticQueryHashes":[]}